{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1542cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nischay/spark/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/03/31 23:31:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover, VectorAssembler\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName('FinalProject').getOrCreate()\n",
    "\n",
    "# Get sparkcontext from \n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df_business = spark.read.json('/Volumes/Nischay HDD/yelp_academic_dataset_business.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c884b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA Business count: 5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:===========================================================(8 + 0) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_business_MA = df_business.select('business_id', 'name',  'address', 'city', \\\n",
    "                                      'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'categories') \\\n",
    "                                .filter( (df_business.city == 'Vancouver') & (df_business.categories.contains('Restaurants') ) )\n",
    "\n",
    "business_count = df_business_MA.count()\n",
    "print(f'MA Business count: {business_count}')\n",
    "\n",
    "# Sampling data to run on local\n",
    "sampled_business_MA = df_business_MA.sample(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4352e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_business_MA.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost/food_hunter_development',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "      dbtable='restaurants',\n",
    "      user='root',\n",
    "      password='Nanu1996').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcf1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa644cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2483"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_business_MA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA Business count: 5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Reviews count: 16472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                (0 + 8) / 28][Stage 22:>                 (0 + 0) / 8]\r"
     ]
    }
   ],
   "source": [
    "# SCRIPT YOU NEED \n",
    "\n",
    "\n",
    "# ERADING ALL JSON FROM LOCAL\n",
    "\n",
    "df_business = spark.read.json('/Volumes/Nischay HDD/yelp_academic_dataset_business.json')\n",
    "df_reviews  = spark.read.json('/Volumes/Nischay HDD/yelp_academic_dataset_review.json')\n",
    "df_users  = spark.read.json('/Volumes/Nischay HDD/yelp_academic_dataset_user.json')\n",
    "\n",
    "\n",
    "# FILTERING DATASET TO VANCOUVER\n",
    "\n",
    "df_business_MA = df_business.select('business_id', 'name',  'address', 'city', \\\n",
    "                                      'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'categories') \\\n",
    "                                .filter( (df_business.city == 'Vancouver') & (df_business.categories.contains('Restaurants') ) )\n",
    "\n",
    "business_count = df_business_MA.count()\n",
    "print(f'MA Business count: {business_count}')\n",
    "\n",
    "# Sampling data to run on local\n",
    "sampled_business_MA = df_business_MA.sample(0.05)\n",
    "\n",
    "\n",
    "# FETCHING USER REVIEWS FOR FILTERED BUSINESES\n",
    "\n",
    "df_reviews_MA = df_reviews.join(sampled_business_MA, on = 'business_id', how = 'inner') \\\n",
    "                          .select(df_reviews.business_id, df_reviews.user_id, df_reviews.review_id, df_reviews.stars)\n",
    "df_reviews_count = df_reviews_MA.count()\n",
    "print(f'Business Reviews count: {df_reviews_count}')\n",
    "\n",
    "\n",
    "\n",
    "# FETCHING USERS\n",
    "df_users_MA = df_users.join(df_reviews_MA, on = 'user_id', how = 'inner') \\\n",
    "                          .select(df_users.user_id, df_users.name, df_users.review_count, df_users.yelping_since, \\\n",
    "                                  df_users.useful, df_users.funny , df_users.cool , df_users.fans , df_users.average_stars)\n",
    "df_users_MA.show(2)\n",
    "\n",
    "df_users_MA_count = df_users_MA.count()\n",
    "print(f'Users count: {df_users_MA_count}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WRITRING THE DFs into paraquet\n",
    "\n",
    "sampled_business_MA.coalesce(1).write.parquet('New_Small_Datasets/small_business_datset')\n",
    "df_reviews_MA.coalesce(1).write.parquet('New_Small_Datasets/small_reviews_dataset')\n",
    "df_users_MA.coalesce(1).write.parquet('New_Small_Datasets/small_users_dataset')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea269364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data generated from above process\n",
    "business_df = spark.read.parquet('New_Small_Datasets/small_business_datset')\n",
    "business_count = business_df.count()\n",
    "\n",
    "reviews_df = spark.read.parquet('New_Small_Datasets/small_reviews_dataset')\n",
    "reviews_count = reviews_df.count()\n",
    "\n",
    "users_df = spark.read.parquet('New_Small_Datasets/small_users_dataset')\n",
    "users_count = users_df.count()\n",
    "\n",
    "print(f'Business COunt: {business_count}')\n",
    "print(f'Reviews COunt: {reviews_count}')\n",
    "print(f'Users COunt: {users_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
